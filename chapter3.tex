\chapter{Station Keeping of AUVs with Reinforced learning} \label{chap:station-keeping}
Traditional station-keeping of AUVs and ROVs are usually based around \textbf{PID}, proportional-integral-derivative, algorithms. In all simplicity the PID controller is a control-loop feedback algorithm, which continuously calculates an error value between a specified desired position and the actual position of the vehicle. The PID algorithm is given in equation \ref{eq:pid}, where $K_{p}$, $K_{i}$ and $K_{d}$ denote the coefficients for the proportional, integral and derivative gains, respectively. 
\begin{equation}
\centering
{\displaystyle \tau = K_{\text{p}}e(t)+K_{\text{i}}\int _{0}^{t}e(\tau )d\tau +K_{\text{d}}{\frac {d(t)}{dt}}}
\label{eq:pid}
\end{equation}
The algorithm uses the \textbf{proportional} term to reduce the error, the \textbf{integral} term to remove possible stationary deviation and the \textbf{derivative} term takes the error values rate of change into account, and thereby tries to compensate for future trends of error.\\\\
To accomplish a sufficient PID algorithm for a given system the controller gains needs to be tuned. The methods of tuning a PID algorithm are many \cite{Fossen}, but they usually surrounds using the mass- and damping-matrices of the system, as well as defining the natural frequency, $\omega_{n}$, and the natural damping, $\zeta$, resulting in \textbf{constant} gains. However, the underwater environment is complex and the autonomous control of AUVs is nonlinear, because of their motions being highly affected by both flow and hydraulic resistance. This results in accurate control being very difficult, especially when the PID algorithm uses constant gains.\\\\
During the recent years there has been a rapid development of artificial intelligence, and machine learning is now being widely used within control theory. The overall goal of using AI in control is to solve complex tasks from unprocessed, high-dimensional and sensory input \cite{Lillicrap}, and several propositions has been made for doing this. David Silver et al. proposed using a \textbf{DPG}, Deterministic Policy Gradient algorithm, for performing complex tasks with high dimensionality, which showed significantly better performance than using stochastic policy gradients as well as having usage on nonlinear optimisation problems \cite{David}. Furthermore Yu Runsheng et al. proposed in 2017 the use of a \textbf{DDPG}, Deep Deterministic Policy Gradient, for trajectory tracking control of AUVs \cite{Yu}, which showed significant improvement compared to the traditional PID controllers. The experiments done in \cite{Yu} and \cite{David} are therefore used as a starting point for accomplishing station-keeping of the \textbf{BlueROV2}.
\section{Deep Deterministic Policy Gradient algorithm}
The Deep Deterministic Policy Gradient (DDPG) algorithm is an \textbf{off-policy}, \textbf{model-free}, \textbf{policy-gradient} and \textbf{actor-critic} algorithm \cite{Emami}. The fact that it is a \textit{policy-gradient} means that it tries to optimise a \textit{policy}, which was defined in chapter \ref{chap: deep}, end-to-end by computing noisy estimates from the gradient of the expected reward from following that specific policy, and then updating the policy in the gradient direction. By being \textit{actor-critic} we recall from chapter \ref{chap: deep} that the policy function (\textit{actor}) is independent of the value function (\textit{critic}). The policy function takes an action in the current state, and the value function computes a \textit{Temporal Difference} (TD) error signal based on the current state and the reward from taking that action. Furthermore, the algorithm is \textit{off-policy} meaning that the policy function is updated every time-step, without making any assumptions if whether or not the agent are following the actual policy.\\\\Finally, by being \textit{model-free} the algorithm do not know anything about the underlying dynamics of how the AUV interacts with the environment. The benefit of doing this is that the mode-free algorithms will directly estimate an optimal policy or a value function through the use of a policy iteration or value iteration, which reduces the computational power needed significantly. \textbf{COMPARE}. Obviously it is more complex to have a model-free approach, in the sense that we require a larger number of training examples, so starting of with a good approximation of the underlying model of the environment is beneficial. However, if the approximation is far off it will only make the problem worse.
\subsection{Exploration and Exploitation}
An important note when dealing with DDPG algorithms, and Reinforcement Learning in general, is the relationship between \textbf{exploration} and \textbf{exploitation}. This deals with the trade-off between exploitation, meaning that the agent uses the "learned" best action to take in a given state, and exploration, meaning that the agent "tries" a new action in a given state. This deals with the fact that although the agent has found what it believes to be the optimal action in a given state, there could always exist an unexplored action that is better. The possibility for exploration is therefore implemented in order to avoid a sub-optimal policy.\\\\
The trade-off between exploration and exploitation is usually determined by a parameter, $\epsilon$. If $\epsilon \longrightarrow 0$ the agent will only exploit previous learned optimal policies, and if $\epsilon \longrightarrow 1$ the agent will always explore new actions in a given state. When the agent is starting to explore the environment, it will have no knowledge about it. This means that it should both explore and exploit actions. However, as the agent discovers more and more about the environment it will increase the certainty of the policy being the optimal policy. This means that $\epsilon$ should decrease as the knowledge about the environment increases. 





