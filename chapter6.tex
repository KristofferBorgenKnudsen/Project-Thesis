\chapter{Conclusion}
This report has investigated the use of \textit{Deep Learning} techniques in \textit{Station Keeping} of underwater vehicles. Traditional methods of Station Keeping have been presented, and it was discussed how Deep Learning techniques could solve the problems related to pose estimation and underwater control. A literature review was conducted on the topics of \textit{machine learning} and \textit{visual servoing}. This revealed that the underwater environment can be defined as a \textit{Markov Decision Process}, which showed that underwater control problems is \textit{solvable} through the use of \textit{Reinforcement Learning}. Investigating the research done on the topic of visual servoing it was revealed that \textit{Convolutional Neural Networks} have showed the most promising results for implementation of visual servoing in underwater robotics.\\\\
In the second part of the report a development of a state-of-the-art \textit{Deep Deterministic Policy Gradient} controller for underwater control was conducted. By investigating the dynamics, kinematics and hydrodynamics of the BlueROV2 it was revealed that a combination of a \textit{PD} controller with the \textit{DDPG} controller could be a possible solution to achieve Station Keeping in \textit{all} 6 DOFs for the vehicle.\\\\
By using the \textit{Gazebo} and \textit{ROS} simulation environments, the results showed that the DDPG algorithm was successful in finding the optimal policy to reach the desired pose in all 6 DOFs. The simulations also revealed the importance of optimal design of the \textit{reward function}, which in all simplicity sets the operation framework for the vehicle. As stated throughout this study, the DDPG algorithm is always looking for the policy that results in the highest total cumulative reward. This means that the algorithm will always discover possible "flaws" in the framework definitions, that the human eye might not see. From the simulation results this was revealed when evaluating the performance of algorithm \ref{alg:reward_func_2}. The algorithm was successfully in reaching the desired pose, but had no stored policy for which actions to execute after reaching this state. This was due to the definition of the \textit{terminal state}, and a possible action to resolve the problem was to include \textit{time} constraints in the reward function. 
