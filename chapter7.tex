\chapter{Future Work}
In chapter \ref{ch:results} the DDPG algorithm, using the reward function given in algorithm \ref{alg:reward_func_2}, was \textit{successful} in finding an optimal policy for reaching the desired Station Keeping pose. However, it was revealed that the framework definition resulted in difficulties for the vehicle to do Station Keeping at this pose. A possible implementation in future work could therefore be to add a \textit{time} constraint in the reward function. This is done to make sure that the terminal state is not reached until the vehicle has stayed at the desired pose for a sufficient time period.\\\\
Furthermore, it should be performed a comparison study between the performance of the DDPG algorithm and \textit{classical} control techniques, as well as other algorithms in the \textit{Reinforcement Learning} family. In example, Kjaernli \cite{Kjaernli} concluded in 2018 that a \textit{Proximal Policy Optimization} (PPO) algorithm had shown feasible results in dynamic positioning of the BlueROV2, which could be a possible implementation in future work. In this report, a \textit{dual} control design was suggested, where the PD algorithm controlled the states ($z, \phi, \theta$) and the DDPG algorithm controller the state ($x, y, \psi$). An interesting approach in a future work could therefore be to investigate the performance when removing the PD algorithm, and try to control all 6 DOFs with Reinforcement Learning.\\\\
From the equation of motions in chapter \ref{chap:modeling} it was stated that environmental forces, including current, wind and wave forces, were neglected for simplicity. In the real-life scenario this is obviously not the case, and especially the current velocities will impact the performance of the vehicle. A future approach would therefore be to add a current force into the simulation environment. 
