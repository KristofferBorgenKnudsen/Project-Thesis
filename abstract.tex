\section*{Abstract}
This report investigates the possibilities of applying machine learning techniques, more specifically \textit{Deep Learning}, in \textit{Station Keeping} of underwater vehicles.\\\\The usage of Deep Learning in this report both surrounds estimation of the vehicles \textit{pose}, meaning position and orientation, as well as controller design. For pose estimation the report focuses on the possibilities of \textit{Visual Servoing}, mainly applying this technique through the use of \textit{Convolutional Neural Networks} (CNN). Controller design is based on the principles of \textit{Reinforcement Learning}, and an implementation of a \textit{Deep Deterministic Policy Gradient} (DDPG) algorithm for control of an \textit{Autonomous Underwater Vehicle} was conducted.\\\\
In traditional Station Keeping methods, the global pose of underwater vehicles is usually obtained by on-board inertial navigation systems (INS) and acoustic positioning systems. However, there is a lot of uncertainty in these measurements, which has motivated the use of visual-based techniques to extract the vehicles pose. Furthermore, the process of doing underwater control is a difficult task. This is mainly due to the complex underwater environment making the autonomous control nonlinear, since the vehicles motions are easily influenced by flow and hydraulic resistance. This, together with the rapid developments in artificial intelligence, has triggered the interest of using machine learning techniques in underwater control.\\\\
In order to accomplish Station Keeping a dynamic model of the BlueROV2 was used, which is a state-of-the-art \textit{Remotely Operated Vehicle} (ROV). The BlueROV2 was controlled in all 6 degrees of freedom (DOFs), through suggesting the use of \textit{dual} control design with a \textit{Proportional-Derivative} (PD) algorithm and a DDPG algorithm. The dual controller was then evaluated on the BlueROV2, in combination with the simulation environments \textit{Gazebo} and \textit{Robot Operating System} (ROS), as well as the machine learning environment \textit{TensorFlow}. In simulation the BlueROV2 was not connected by a tether, and it is therefore assumed that the vehicle can be simulation as an AUV.\\\\
The simulation results showed that it was possible to sufficiently train the DDPG algorithm to accomplish a desired pose with the dual control design, in all 6 DOFs. However, it also revealed the importance of optimal design of the \textit{reward function}, which in all simplicity defines the operation framework for the algorithm. Due to necessity of defining a \textit{terminal state} in order to prove convergence of the optimal policy, this resulted in difficulties in keeping the vehicle at the desired pose. A suggestion to resolve these difficulties was to introduce a \textit{time} constraint in the reward function. 